---
title: "subset_modeling"
output: html_document
---

In this portion of the analysis, we will perform modeling on a smaller subset of data in attempt to improve model performance. 

# load packages and data
```{r}
library(ggplot2) #for plotting
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(tidymodels) #for modeling
library(rpart)
library(glmnet)
library(ranger)
library(rpart.plot)  # for visualizing a decision tree
library(vip)         # for variable importance plots

#path to data

#load cleaned USBSET data. 
data <- readRDS(here("data", "processed_data", "data_subset.rds"))
```

# split data into train and test subsets
```{r}
# set seed for reproducible analysis (instead of random subset each time)
set.seed(123)
#subset 3/4 of data as training set
data_split <- initial_split(data, 
                            prop = 7/10, 
                            strata = particles_l) #stratify by MP concentration for balanced outcome

#save sets as data frames
train_data <- training(data_split)
test_data <- testing(data_split)
```

# Cross validation
We want to perform 5-fold CV, 5 times repeated
```{r}
#create folds (resample object)
set.seed(123)
folds <- vfold_cv(train_data, 
                  v = 5, 
                  repeats = 5,
                  strata = particles_l) #folds is set up to perform our CV

#linear model set up
lm_mod <- linear_reg() %>% 
            set_engine('lm') %>% 
            set_mode('regression')

#create recipe for data and fitting and make dummy variables
MP_rec <- recipe(particles_l ~ ., data = train_data) %>% step_dummy(all_nominal())

#workflow set up
MP_wflow <- 
  workflow() %>% add_model(lm_mod) %>% add_recipe(MP_rec)

#use workflow to prepare recipe and train model with predictors
MP_fit <- 
  MP_wflow %>% fit(data = train_data)

#extract model coefficient
MP_fit %>% extract_fit_parsnip() %>% tidy()
```

# Null model performance
```{r}
#recipe for null model
null_train_rec <- recipe(particles_l ~ 1, data = train_data) #predicts mean of outcome

#null model workflow incorporating null model recipe
null_wflow <- workflow() %>% add_model(lm_mod) %>% add_recipe(null_train_rec)

# I want to check and make sure that the null model worked as it was supposed to, so I want to view the predictions and make sure they are all the mean of the outcome
#get fit for train data using null workflow
nullfittest <- null_wflow %>% fit(data = train_data)
#get predictions based on null model
prediction <- predict(nullfittest, train_data)
test_pred <- predict(nullfittest, test_data)
#the predictions for the train and test data are all the same mean value, so this tells us the null model was set up properly

#Now, we'll use fit_resamples based on the tidymodels tutorial for CV/resampling (https://www.tidymodels.org/start/resampling/)
#fit model with training data
null_fit_train <- fit_resamples(null_wflow, resamples = folds)

#get results
metrics_null_train <- collect_metrics(null_fit_train)
#RMSE for null train fit is 109.4

#repeat for test data
null_test_rec <- recipe(particles_l ~ 1, data = test_data) #predicts mean of outcome
null_test_wflow <- workflow() %>% add_model(lm_mod) %>% add_recipe(null_test_rec) #sets workflow with new test recipe
null_fit_test <- fit_resamples(null_test_wflow, resamples = folds) #performs fit
metrics_null_test <- collect_metrics(null_fit_test) #gets fit metrics
#RMSE for null test fit is 109.4
```

# Model tuning and fitting
Include:
1. Model specification
2. Workflow definition
3. Tuning grid specification
4. Tuning w/ cross-validation + `tune_grid()`

## LASSO model
```{r}
#based on tidymodels tutorial: case study
#cross validation
set.seed(123)
cell_folds <- vfold_cv(train_data)

#model specification
lasso <- linear_reg(penalty = tune()) %>% set_engine("glmnet") %>% set_mode("regression")

#set workflow
lasso_wf <- workflow() %>% add_model(lasso) %>% add_recipe(MP_rec)

#tuning grid specification
lasso_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))

#tuning with CV and tune_grid
lasso_res <- lasso_wf %>% tune_grid(resamples = cell_folds, 
                                    grid = lasso_grid, 
                                    control = control_grid(save_pred = TRUE), 
                                    metrics = metric_set(rmse))
#view model metrics
lasso_res %>% collect_metrics()

#select top models
top_lasso <- 
  lasso_res %>% show_best("rmse") %>% arrange(penalty)
top_lasso #view

#see best lasso
best_lasso <- lasso_res %>% select_best()
best_lasso #view

#finalize workflow with top model
lasso_final_wf <- lasso_wf %>% finalize_workflow(best_lasso)

#fit model with finalized WF
lasso_fit <- lasso_final_wf %>% fit(train_data)
```

### LASSO plots
```{r}
#diagnostics
autoplot(lasso_res)
#calculate residuals
lasso_resid <- lasso_fit %>%
  augment(train_data) %>% #this will add predictions to our df
  select(.pred, particles_l) %>%
  mutate(.resid = particles_l - .pred) #manually calculate residuals

#model predictions from tuned model vs actual outcomes
lasso_pred_plot <- ggplot(lasso_resid, aes(x = particles_l, y = .pred)) + geom_point() + 
  labs(title = "Predictions vs Actual Outcomes: LASSO", x = "Particles/L Outcome", y = "Particles/L Prediction")
lasso_pred_plot


#plot residuals vs predictions
lasso_resid_plot <- ggplot(lasso_resid, aes(y = .resid, x = .pred)) + geom_point() + 
  labs(title = "Predictions vs Residuals: LASSO", x = "Particles/L Prediction", y = "Residuals")
lasso_resid_plot #view plot

#compare to null model
metrics_null_train #view null RMSE for train data
lasso_res %>% show_best(n=1) #view RMSE for best lasso model

```

next I want to see what predictors are used in the best lasso model. also need to compare to null model

RMSE for best LASSO model on the subset data is 113.09, compared to 109.4 RMSE for the null model. This is definitely not a well-performing model, since we want a better RMSE for any model we build than the RMSE for the null model. 

In the perfect outcome/prediction plot, you hope to see data scattered along a 45 degree diagonal line. Instead, data is clustered. However, the scales of the x and y axis are different which may impact how we are viewing the data. Let's visualize with a better scale.

```{r}
#model predictions from tuned model vs actual outcomes SCALE ADJUSTED
lasso_pred_plot_adj <- ggplot(lasso_resid, aes(x = particles_l, y = .pred)) + geom_point() + 
  labs(title = "Predictions vs Actual Outcomes: LASSO", x = "Particles/L Outcome", y = "Particles/L Prediction") + ylim(0, 1000) + xlim(0,1000)
lasso_pred_plot_adj
```

Now that we have adjusted the scale, our outcome vs prediction plot looks decent, and there don't appear to be systematic deviations. This indicates that our LASSO model may be flexible enough for our data. 

#plot for how the number of predictors included in the LASSO model changes with the tuning parameter
```{r}
x <- lasso_fit$fit$fit$fit 
plot(x, "lambda")

summary(best_lasso)

coefficients <- lasso_fit %>% extract_fit_parsnip() %>% tidy()
lasso_fit %>% extract_fit_parsnip() %>% tidy()

```

Try modeling with data subset, including these variables: "particles_l", "visual_score", "turbidity.ntu", "temperature.c", "e.coli.cfu", "population", "dist".


The results we get from the initial machine learning modeling and from the modeling of this subset data are not great. Let's think of some ways that we can go about improving our model. 
- An option is variable/predictor removal. We tried that with the subset data. We could attempt further subsets going forward. 
- Variable transformation. Some microplastic studies use log transformation on the concentration values to get a normal distribution. 
- Alternatives: account for different distribution. The distribution of microplastic concentration may be considered more Poisson distributed than normally distributed. First, we can test to see whether it falls in either of these distribution categories, and based on the results we can apply a GLM with Poisson distribution. (Tranformation could make intrepretability more difficult - may want to avoid)
- however, regularization may accomplish our goals more efficiently - we are advised that subset selection is not always a great idea and doesn't neccesarily do the best job of addressing overfitting
- could try ridge regression for alternate form of regularization 
- could try decision tree (though we learned these usually don't perform as well as LASSO)

```{r}
#check for Poisson distribution
#https://stackoverflow.com/questions/59809960/how-do-i-know-if-my-data-fit-a-poisson-distribution-using-r
dispersion_test <- function(x) 
{
  res <- 1-2 * abs((1 - pchisq((sum((x - mean(x))^2)/mean(x)), length(x) - 1))-0.5)

  cat("Dispersion test of count data:\n",
      length(x), " data points.\n",
      "Mean: ",mean(x),"\n",
      "Variance: ",var(x),"\n",
      "Probability of being drawn from Poisson distribution: ", 
      round(res, 3),"\n", sep = "")

  invisible(res)
}

set.seed(1)
x <- data$particles_l

dispersion_test(x)
```






```